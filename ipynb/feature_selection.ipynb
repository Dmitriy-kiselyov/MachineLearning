{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы отбора признаков\n",
    "\n",
    "Это процесс отбора подмножества значимых признаков для использования в построении модели. Методы отбора применяют в случаях когда нужно сократить количество признаков для:\n",
    "- ускорения алгоритмов обучения\n",
    "- уменьшения потребляемой памяти\n",
    "- избавления от неинформативных признаков и выбросов\n",
    "\n",
    "Прежде, чем описывать алгоритмы, подготовим среду *jupyter notebook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И установим внешние зависимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также понадобятся вспомогательные методы.\n",
    "\n",
    "1. Получает на вход выборку `dataset`. Ожидается, что `dataset` будет взят из `sklearn/datasets`, но можно любую структуру данных с аналогичным интерфейсом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_count(dataset):\n",
    "    return len(dataset.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Получает на вход выборку `dataset` и массив прикнаков `feature`. Выбирает из выборки только признаки из `feature`, проводит по ним обучение (взял *наивного Байеса*), проводит классификацию и возвращает количество допущенных ошибок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def count_error(dataset, features):\n",
    "    data = dataset.data[:, features]\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(data, dataset.target)\n",
    "\n",
    "    y_predict = gnb.predict(data)\n",
    "\n",
    "    return (dataset.target != y_predict).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полный перебор\n",
    "\n",
    "Полный перебор даёт 100% гарантию наилучшего ответа. Очевидно, что сложность алгоритма *экспоненциальная* и при большом количестве признаков умноженном на скорость обучения ответа можно не дождаться.\n",
    "\n",
    "Алгоритм перебирает всевозможные наборы прикнаков и из наилучшего берет с наименьшим количеством признаков.\n",
    "\n",
    "Введём вспомогательную функцию, которая будет по количеству признаков возвращать генераторы комбинаций прикнаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __feat_gen(count):\n",
    "    for c in range(1, count + 1):\n",
    "        arr = list(range(count))\n",
    "        comb = combinations(arr, c)\n",
    "\n",
    "        yield from comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция полного перебора признаков на вход получает выборку `dataset`, возвращает словарь с интерфейсом:\n",
    "- `features` – массив признаков, дающий наименьшую ошибку\n",
    "- `error` – ошибка\n",
    "\n",
    "**Все последующие функции отбора признаков имеют такой же тип ответа.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_full_search(dataset):\n",
    "    feature_count = get_feature_count(dataset)\n",
    "    result = {\n",
    "        \"error\": 9999,\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    for feat_cur in __feat_gen(feature_count):\n",
    "        error = count_error(dataset, feat_cur)\n",
    "\n",
    "        if error < result[\"error\"]:\n",
    "            result = {\n",
    "                \"error\": error,\n",
    "                \"features\": feat_cur\n",
    "            }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Применение и сравнение результатов будет в конце**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм ADD\n",
    "\n",
    "Является жадным алгоритмом. Начинает с набора признаков `= []`. Добавляет в набор по одному наилучшему признаку до тех пор, пока качество алгоритма не начинает ухудшаться.\n",
    "\n",
    "Второй параметр алгоритма введён только для **алгоритма ADD-DEL**. Про него можно прочитать ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_add(dataset, result=None):\n",
    "    feature_count = get_feature_count(dataset)\n",
    "    if result is None:\n",
    "        result = {\n",
    "            \"error\": 9999,\n",
    "            \"features\": []\n",
    "        }\n",
    "\n",
    "    while True:\n",
    "        result_cur = result\n",
    "\n",
    "        for feat in range(feature_count):\n",
    "            if feat in result[\"features\"]:\n",
    "                continue\n",
    "\n",
    "            features = result[\"features\"] + [feat]\n",
    "            features.sort()\n",
    "\n",
    "            error = count_error(dataset, features)\n",
    "            if error <= result_cur[\"error\"]:\n",
    "                result_cur = {\n",
    "                    \"error\": error,\n",
    "                    \"features\": features\n",
    "                }\n",
    "\n",
    "        if result_cur == result:\n",
    "            break\n",
    "\n",
    "        result = result_cur\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм DEL\n",
    "\n",
    "Является жадным алгоритмом. Начинает с набора признаков = полному количеству признаков.\n",
    "На каждом шаге удаляет по одному наихудшему признаку до тех пор, пока качество алгоритма не ухудшается.\n",
    "\n",
    "Второй параметр алгоритма введён только для **алгоритма ADD-DEL**. Про него можно прочитать ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_del(dataset, result=None):\n",
    "    feature_count = get_feature_count(dataset)\n",
    "    if result is None:\n",
    "        features_all = list(range(feature_count))\n",
    "        result = {\n",
    "            \"error\": count_error(dataset, features_all),\n",
    "            \"features\": features_all\n",
    "        }\n",
    "\n",
    "    while len(result[\"features\"]) > 1:\n",
    "        result_cur = result\n",
    "\n",
    "        for feat in result[\"features\"]:\n",
    "            features = result[\"features\"][:]\n",
    "            features.remove(feat)\n",
    "\n",
    "            error = count_error(dataset, features)\n",
    "            if error <= result_cur[\"error\"]:\n",
    "                result_cur = {\n",
    "                    \"error\": error,\n",
    "                    \"features\": features\n",
    "                }\n",
    "\n",
    "        if result_cur == result:\n",
    "            break\n",
    "\n",
    "        result = result_cur\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм ADD-DEL\n",
    "\n",
    "Является комбинацией **ADD** и **DEL**. Начинает с набора признаков `= []`. Далее итеративно сначала применяет **ADD** алгоритм, потом **DEL** алгоритм. Эти шаги повторяются итеративно, пока качество алгоримта растёт. Таким образом, является небольшим улучшением **ADD** алгоритма, позволяя избавить от \"лишне собранных\" признаков и продолжить использовать жадный метод выбора признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_add_del(dataset):\n",
    "    result = {\n",
    "        \"error\": 9999,\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        new_result = selection_add(dataset, result)\n",
    "        new_result = selection_del(dataset, new_result)\n",
    "\n",
    "        if new_result[\"error\"] >= result[\"error\"]:\n",
    "            break\n",
    "\n",
    "        result = new_result\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм также можно начать и с **DEL** метода, потом применяя **ADD**. Поменяется только изначальная выборка. Она будет не пустая, а содержать все признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение Полного перебора, ADD, DEL, ADD-DEL\n",
    "\n",
    "Сравним результаты редукции выборки разных алгоритмов на выборке `breast_cancer` по первым 12 признакам из библиотеки `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полный набор признаков\n",
      "Количество ошибок:  48\n",
      "Количество признаков:  12\n",
      "Признаки:  [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "-------------------------------------------\n",
      "Полный перебор\n",
      "Количество ошибок:  35\n",
      "Количество признаков:  5\n",
      "Признаки:  [0 1 4 6 8]\n",
      "-------------------------------------------\n",
      "Алгоритм ADD\n",
      "Количество ошибок:  36\n",
      "Количество признаков:  4\n",
      "Признаки:  [ 0  1  7 11]\n",
      "-------------------------------------------\n",
      "Алгоритм DEL\n",
      "Количество ошибок:  40\n",
      "Количество признаков:  3\n",
      "Признаки:  [0 1 7]\n",
      "-------------------------------------------\n",
      "Алгоритм ADD-DEL\n",
      "Количество ошибок:  36\n",
      "Количество признаков:  4\n",
      "Признаки:  [ 0  1  7 11]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "feature_count = 12\n",
    "dataset.data = dataset.data[:, :feature_count]\n",
    "\n",
    "\n",
    "def __log(message, result):\n",
    "    print(message)\n",
    "    print(\"Количество ошибок: \", result[\"error\"])\n",
    "    print(\"Количество признаков: \", len(result[\"features\"]))\n",
    "    print(\"Признаки: \", np.asarray(result[\"features\"]))\n",
    "    print(\"-------------------------------------------\")\n",
    "\n",
    "\n",
    "features_all = list(range(feature_count))\n",
    "__log(\"Полный набор признаков\", {\n",
    "    \"error\": count_error(dataset, features_all),\n",
    "    \"features\": features_all\n",
    "})\n",
    "\n",
    "__log(\"Полный перебор\", selection_full_search(dataset))\n",
    "\n",
    "__log(\"Алгоритм ADD\", selection_add(dataset))\n",
    "\n",
    "__log(\"Алгоритм DEL\", selection_del(dataset))\n",
    "\n",
    "__log(\"Алгоритм ADD-DEL\", selection_add_del(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно увидеть, **полный перебор** является лидером по наименьшей ошибке. Алгориты **ADD**, **DEL** и **ADD-DEL** дали\n",
    "близкий к **полному перебору** ответ. Маленький разрыв легко объяснить небольшим количеством фичей, маленькой выбркой и\n",
    "тривиальностью выборки. На практике этот разрыв будет большой. Тут важно увидеть, что жадные алгоритмы в погоне за жадностью\n",
    "не увидели, что набор \"менее полезных признаков\" в совокупности дал наи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обхода в глубину (DFS)\n",
    "\n",
    "Этот алгоритм – оптимизация полного перебора признаков с отсечением вариантов. Эвристика такова: на каждом шаге **наращивания** выборки следить, не встречалась ли ранее выборка с меньшей ошибкой и меньшим количеством признаков. И если была, текущую выборку при обходе отбрасываем, не пытаясь ее улучшать.\n",
    "\n",
    "Алгоритм эвристический, поэтому за счет улучшения сложности даёт ошибку. И качество результата зависит от того, в каком порядке расположены признаки. Обязательной частью алгоритма является сортировка признаков по *полезности* перед началом обхода. Без этой сортировки алгоритм даёт результаты гораздо хуже и не имеет большого смысла.\n",
    "\n",
    "*В рамках алгоритма условимся, что фича – это словарь с полями `{i: номер признака, pos: порядковый номер признака при сортировке полезности}`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __sort_features(dataset):\n",
    "    feature_count = get_feature_count(dataset)\n",
    "    feature_errors = list(map(lambda i: {\"i\": i, \"error\": count_error(dataset, [i])}, range(feature_count)))\n",
    "    features_sorted = list(sorted(feature_errors, key=lambda x: x[\"error\"]))\n",
    "    return list(map(lambda f, pos: {\"i\": f[\"i\"], \"pos\": pos}, features_sorted, range(feature_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы в дереве обхода не было одинаковых наборов признаков, добавляем на этапе **наращивания** только те признаки, что больше текущих по коэффициенту.\n",
    "\n",
    "*Функция получения наибольшего порядкового номера выборки:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __max_feature_pos(features):\n",
    "    return max(features, key=lambda f: f[\"pos\"])[\"pos\"] if len(features) != 0 else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Также понадобится функция преобразующая `{i, pos}` в массив номеров признаков*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __flatten_features(features):\n",
    "    return list(map(lambda f: f[\"i\"], features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной частью алгоритма является **наращивание** текущего набора признаков для дальнейшего обхода. Функция рекурсивная и продолжает обход лишь в том случае, если на ранних этапах не было выборки с лучшим показателем. Иначе она запоминает текущий результат и рекурсивно прибавляет к текущей выборки не использованные номера признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __increase(features_cur, features, results, dataset):\n",
    "    length = len(features_cur)\n",
    "    result = results[length]\n",
    "    error = count_error(dataset, __flatten_features(features_cur)) if length != 0 else 9999\n",
    "\n",
    "    for j in range(length):\n",
    "        if results[j][\"error\"] < error:\n",
    "            return\n",
    "\n",
    "    if error < result[\"error\"]:\n",
    "        result[\"error\"] = error\n",
    "        result[\"features\"] = features_cur\n",
    "\n",
    "    max_feature = __max_feature_pos(features_cur)\n",
    "    for feature in features:\n",
    "        if feature[\"pos\"] > max_feature:\n",
    "            __increase(features_cur + [feature], features, results, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно, *сам алгоритм* при таком разбиении на функции становится простым:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_dfs(dataset):\n",
    "    feature_count = get_feature_count(dataset)\n",
    "    features = __sort_features(dataset)\n",
    "\n",
    "    results = list(map(lambda x: {\"error\": 9999, \"features\": []}, range(feature_count + 1)))\n",
    "\n",
    "    __increase([], features, results, dataset)\n",
    "\n",
    "    result = min(results, key=lambda x: x[\"error\"])\n",
    "    result[\"features\"] = sorted(__flatten_features(result[\"features\"]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм обхода в ширину (BFS)\n",
    "\n",
    "Этот алгоритм является улучшением алгоритма **ADD**. Улучшение состоит в том, что на каждой итерации мы запоминаем не один лучший набор признаков, а `iter_limit` наборов. И наращивание происходит не по 1 набору, а сразу по `iter_limit` наборам.\n",
    "\n",
    "В случае, когда `iter_limit = 1`, алгоритм равен **ADD**. Сложность работы алгоритма вырастает в `iter_limit` раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_bfs(dataset, iter_limit=1):\n",
    "    feature_count = get_feature_count(dataset)\n",
    "    result_best = {\n",
    "        \"error\": 9999,\n",
    "        \"features\": []\n",
    "    }\n",
    "\n",
    "    result = list(map(lambda i: {\"features\": [i]}, range(feature_count)))\n",
    "\n",
    "    for iteration in range(1, feature_count + 1):\n",
    "        for res in result:\n",
    "            res[\"error\"] = count_error(dataset, res[\"features\"])\n",
    "\n",
    "        result.sort(key=lambda r: r[\"error\"])\n",
    "\n",
    "        if len(result) > iter_limit:\n",
    "            result = result[:iter_limit]\n",
    "\n",
    "        if result[0][\"error\"] > result_best[\"error\"]:\n",
    "            break\n",
    "\n",
    "        if result[0][\"error\"] < result_best[\"error\"]:\n",
    "            result_best = result[0]\n",
    "\n",
    "        result_new = []\n",
    "        for res in result:\n",
    "            for i in range(feature_count):\n",
    "                if i not in res[\"features\"]:\n",
    "                    result_new.append({\"features\": res[\"features\"] + [i]})\n",
    "\n",
    "        result = result_new\n",
    "\n",
    "    result_best[\"features\"].sort()\n",
    "\n",
    "    return result_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение ADD, DFS, BFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся выборкой `breast_cancer` по всем 30 признакам и сравним 2 алгоритма друг с другом, а также насколько они лучше предыдущих жадных на примере **ADD**.\n",
    "\n",
    "Сравнить с **полным перебором** к сожалению не выйдет из-за большой размерности выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Алгоритм ADD\n",
      "Количество ошибок:  15\n",
      "Количество признаков:  6\n",
      "Признаки:  [11 16 18 21 22 24]\n",
      "-------------------------------------------\n",
      "Поиск в глубину\n",
      "Количество ошибок:  10\n",
      "Количество признаков:  9\n",
      "Признаки:  [ 1 16 17 18 20 21 22 24 27]\n",
      "-------------------------------------------\n",
      "Поиск в ширину\n",
      "Количество ошибок:  12\n",
      "Количество признаков:  7\n",
      "Признаки:  [ 1 16 17 21 23 24 27]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_breast_cancer()\n",
    "feature_count = 30\n",
    "dataset.data = dataset.data[:, :feature_count]\n",
    "\n",
    "__log(\"Алгоритм ADD\", selection_add(dataset))\n",
    "__log('Поиск в глубину', selection_dfs(dataset))\n",
    "__log('Поиск в ширину', selection_bfs(dataset, iter_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поиск в глубину показал себя наиболее эффективным за счёт того, что он пытается оптимизировать полный перебор, а не улучшить жадный алгоритм."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
